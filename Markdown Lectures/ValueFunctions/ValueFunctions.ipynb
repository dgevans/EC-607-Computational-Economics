{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Value Function Iteration\n<style type=\"text/css\">\n  .reveal p {\n    text-align: left;\n  }\n  .reveal ul {\n    display: block;\n  }\n  .reveal ol {\n    display: block;\n  }\n</style>\n## Setup\nStart with generic environment:\n\n* Let $\\beta \\in (0,1)$ be a discount factor\n\n* Can choose a sequence of controls $\\{u_t\\}_{t=0}^\\infty$\n\n* Maximize\n$$\n    \\sum_{t=0}^\\infty \\beta ^t r(x_{t-1},u_t),\n$$\n\n* subject to $x_{t} = g(x_{t-1},u_t)$ with $x_{-1}\\in \\mathbb R^n$ given \n\n* **Note:** Choice of $x_t$ not always obvious will go into examples later\n\n## Bellman Equation Pt 1\n\n* Begin with \"let\": Let \n$$\n    V(x) = \\max_{\\{u_t,x_t\\}} \\sum_{t=0}^\\infty \\beta^t r(x_{t-1},u_t),\n$$ sujbect to $x_t = g(x_{t-1},u_t)$ and $x_{-1} = x$.\n\n* Can break maximization into two parts:\n\\begin{align*}\n    &\\max_{\\{u_t,x_t\\}} \\sum_{t=0}^\\infty \\beta^t r(x_{t-1},u_t)\\\\ = &\\max_{u_0,x_0} r(x_{-1},u_0) + \\beta\\max_{\\{u_t,x_t\\}} \\sum_{t=1}^\\infty \\beta^{t-1} r(x_{t-1},u_t)\\\\ =  &\\max_{u_0} r(x_{-1},u_0) +\\beta V(g(x_{-1},u_0)) \n\\end{align*}\n\n## Bellman Equation Pt 2\n* This gives the following Bellman Equation\n$$\n    V(x)= \\max_{u}  r(x,u) +\\beta V(g(x,u))\n$$\n\n* Want:\n    * Root of this functional equation: $V(x)$\n    * Optimal Policy:\n    $$\n        h(x) = \\arg\\max_{u}  r(x,u) +\\beta V(g(x,u))\n    $$\n\n* Note:\n    $$\n    V(x) = r(x,h(x))+\\beta V(g(x,h(x))\n    $$\n\n## Iterating on the Bellman Equation\n* Starting from an initial function $V_0$\n\n* Define a sequence of functions $V_0,V_1,V_2,\\ldots$ via\n$$\nV_{j+1}(x)= \\max_{u}  r(x,u) +\\beta V_j(g(x,u))\n$$\n\n* With sufficient regularity conditions on $r$ and $g$\n    * Possible to show that in the limit $V_j$ solves the Bellman equation.\n\n* Problem: Generally impossible to do more than one iteration by hand\n\n\n# Analytical Example\n## A simple RBC model\n* Planner chooses $\\{c_t,k_{t}\\}$ to maximize\n$$\n    \\sum_{t=0}^\\infty \\beta^t \\ln(c_t)\n$$ subject to $k_t + c_t = A k_{t-1}^\\alpha$ and $k_{-1}$\n\n\n* Bellman Equation:\n$$\n    V(k) = \\max_c \\ln(c) + \\beta V(Ak^\\alpha - c)\n$$\n\n## Iterating On Bellman Equation\n* Iteration 0: $V^0(k) = 0$\n\n* Iteration 1:$V^1(k) = \\ln(A) + \\alpha \\ln(k)$\n\n* Iteration 2:\n\\begin{align*}V^2(k) =& \\ln\\left(\\frac{A}{1+\\alpha\\beta}\\right)+\\beta\\ln A\\\\& + \\alpha\\beta\\ln\\left(\\frac{\\alpha\\beta A}{1+\\alpha\\beta}\\right)+\\alpha(1+\\alpha\\beta)\\ln k\\end{align*}\n\n* Naturally leads to guess $V^j(k) = E^j + F^j \\ln(k))$\n\n## Iterating On Bellman Equation Pt 2\nSuppose $V^j(k) = E^j + F^j \\ln(k))$ then\n\n* $c^{j+1}(k) = \\frac{1}{1+\\beta F^j}A k^\\alpha$\n\n* $k^{j+1}(k) = \\frac{\\beta F^j}{1+\\beta F^j}A k^\\alpha$\n\n* $F^{j+1} = \\alpha + \\alpha\\beta F^j$\n\n* $E^{j+1} = \\beta E^j + \\ln\\left(\\frac{A}{1+\\beta F^j}\\right)+\\beta F^j\\ln\\left(\\frac{\\beta F^j}{1+\\beta F^j}A\\right)$\n\nTherefore at iteration $\\infty$:\n\\begin{align*}\nV_\\infty(k) = &(1-\\beta)^{-1}\\left\\{\\ln(A(1-\\beta\\alpha))+\\frac{\\alpha\\beta}{1-\\alpha\\beta}\\ln(A\\beta\\alpha)\\right\\}\\\\&+\\frac{\\alpha}{1-\\beta\\alpha}\\ln(k)\n\\end{align*}\n\n\n## Code implementation - Variable setup"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Gadfly,LinearAlgebra\nA = 1\nα = 0.33\nβ = 0.95\nJ = 200 #number of iterations"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code implementation - Bellman Equation"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "E = zeros(J)\nF = zeros(J)\nE[1] = log(A)\nF[1] = α\nfor j in 2:J\n    #apply formula for E\n    E[j] = β*E[j-1] + log(A/(1+β*F[j-1]))+\n           β*F[j-1]*log(β*F[j-1]/(1+β*F[j-1])*A)\n    #apply formula for F\n    F[j] = α + α*β*F[j-1]\nend\nE_infty = (1-β)^(-1) * (log(A*(1-β*α))+\n         (α*β)/(1-α*β)*log(A*β*α))\nF_infty = α/(1-β*α)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Value Functions"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "#Now plot value functions\n#sets some defaults for plotting\nGadfly.push_theme(Theme(major_label_font_size=20pt,minor_label_font_size=14pt,key_label_font_size=16pt,\n                        line_width=2pt,point_size=3pt))\nfig = plot( k->0 ,0.01,0.5,style(line_width=1pt), Guide.XLabel(\"Capital\"),Guide.YLabel(\"Value Function\"))\nfor j in 1:100\n    #Add value function to plot for each iteration j in 1,2,..,100\n    push!(fig, layer(k->E[j]+F[j]*log(k),0.01,0.5,Geom.line,style(line_width=1pt) ))\nend\n#Add limiting value function\npush!(fig,layer(k->E_infty+F_infty*log(k),0.01,0.5,Geom.line,\n                style(default_color=colorant\"red\"),order=1))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Policy Rules"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "#Now plot the policy rules]\nfig = plot( k->0 ,0.01,0.5,Guide.XLabel(\"Capital\"),Guide.YLabel(\"Next Period Capital\"))\n#NOTE only 10 iterations\nfor j in 1:10\n    #Add policy rules to plot for each iteration j in 1,2,..,10\n    push!(fig, layer(k->(β*F[j])/(1+β*F[j])*A*k^α,0.01,0.5,Geom.line,style(line_width=1pt)) )\nend\n#Add limiting value function, note order = 1 puts it on top\npush!(fig,layer(k->(β*F_infty)/(1+β*F_infty)*A*k^α,0.01,0.5,Geom.line,\n                style(default_color=colorant\"red\"),order=1))\n#add line representing kprime = k\npush!(fig,layer(k->k,0.01,0.5,Geom.line,style(line_width=3pt,default_color=colorant\"black\"),order=2))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# McCall Search model\n## Solving the Bellman Equation\n* Generally solutions cannot be computed in closed form\n\n* Require **numerical** techniques to solve for value function and policy rules\n\n* **Problem:** How to store value function in the computer? (Infinite Dimensional Object)\n\n## McCall Search Model\n* Start with the simplest version\n$$\nv(w) = \\max\\left\\{c+\\beta \\int v(w')dF(w'), \\frac{w}{1-\\beta}\\right\\}\n$$\n\n* Again, $v(w)$ is infinite dimensional\n    * but we can simplify things by assuming that F(w') has finite support\n\n    * Suppose there are only $S$ possible wage values\n\n* Suppose $F(w)$ is described by a vector $\\boldsymbol w$ and probabilities $\\boldsymbol p$\n\n    * $\\boldsymbol p[s]$ is the probability of wage $\\boldsymbol w[s]$.\n\n    * I'll use the notation $\\boldsymbol p[s]$ to represent $\\boldsymbol p_s$\n\n\n## Discretized Bellman Equation\n* Value function $v(w)$ can now be described by a vector $\\boldsymbol v$\n    * $\\boldsymbol v[s]$ is the value of having wage offer $\\boldsymbol w[s]$ in hand\n\n* Bellman equation can now be written as$$\n\\boldsymbol v[s] = \\max\\left\\{c+\\beta \\boldsymbol p\\cdot \\boldsymbol v, \\frac{\\boldsymbol w[s]}{1-\\beta}\\right\\}\n$$\n\n* The RHS is a map $T:\\mathbb R^S \\mapsto \\mathbb R^S$ given by\n$$\n    T(\\boldsymbol v) = \\max\\left\\{c+\\beta\\boldsymbol p\\cdot\\boldsymbol v, \\frac{\\boldsymbol w[s]}{1-\\beta}\\right\\}\n$$\n\n* Solution to the Bellman Equation is a vector $\\boldsymbol v\\in \\mathbb R^S$ such that\n$$\n    \\boldsymbol v = T(\\boldsymbol v) \n$$\n\n* Want to write this as a function on the computer\n\n## Coding the Bellman Map"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "\"\"\"\n    mccallbellmanmap(v,w,π,c,β)\n\nIterates the McCall search model bellman equation for with value function v.\nReturns the new value function.\n\n# Arguments\n* `v` vector of values for each wage\n* `w` vector of wages\n* `p` vector of probabilities for each wage\n* `c` unemployment benefits\n* `β` time discount factor\n\n\"\"\"\nfunction mccallbellmanmap(v,  w,p,c,β)\n    #first compute value of rejecting the offer\n    v_reject = c + β * dot(p,v) #note that this a Float (single number)\n    #now compute value of accepting the wage offer\n    v_accept = w/(1-β)\n    \n    #finally compare the two\n    v_out = max.(v_reject,v_accept)\n    #this is equivalent to\n    S = length(w)\n    for s in 1:S\n        v_out[s] = max(v_reject,w[s]/(1-β))\n    end\n    \n    return v_out\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why did I write all that documentation......\n* Helps you understand code in the future\n\n* Will help us in grading, make sure we know what you indended to do\n\n* When your writing long files you can look up how you intended the function to be called\n\n* You can get the docstring of a function by typing `?mccallbellmanmap` into the REPL\n\n## Iterating on the Bellman Map"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "S = 40 #number of grid points\n#uniform wage distribution between 1 10\nw = LinRange(1,10,S)\np = ones(S)/S\nβ = 0.96 #lower β will mean code will converge faster\nc = 3\nv0 = zeros(S)\n\nJ = 50 #iterate code J times\nV = zeros(J,S)\nV[1,:] = mccallbellmanmap(v0,w,p,c,β)\nfor j in 2:J\n    V[j,:] = mccallbellmanmap(V[j-1,:],w,p,c,β)\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solving the Bellman Equation\n* We can keep iterating the value function forever\n    * Getting arbtrarily close\n    * How do we know when to stop?\n* Want to have some stopping criteria\n* Will stop iterating when value functions are close enough\n    * i.e. when $\\|\\boldsymbol v- T(\\boldsymbol v)\\|<\\epsilon$\n* Can write a function to do this for us\n\n## Solve McCall Code"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "\"\"\"\n    solvemccall(w,π,c,β[,ϵ])\n\nIterates the McCall search model bellman equation until convergence criterion \nϵ is reached\n\n# Arguments\n* `w` vector of wages\n* `p` vector of probabilities for each wage\n* `c` unemployment benefits\n* `β` time discount factor\n* `ϵ' Stopping criteria (default 1e-6)\n\"\"\"\nfunction solvemccall(w,p,c,β,ϵ=1e-6)\n    #initialize\n    v = w/(1-β)\n    diff = 1.\n    #check if stoping criteria is reached\n    while diff > ϵ\n        v_new = mccallbellmanmap(v,w,p,c,β)\n        #use supremum norm\n        diff = norm(v-v_new,Inf)\n        v = v_new #reset v\n    end\n    return v\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test The Solution"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "v = solvemccall(w,p,c,β)\nprintln(v - mccallbellmanmap(v,w,p,c,β))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The RBC Model\n## Discretizing the Value Function\n* Impose that capital can only take one of $N$ possible values: $\\{k_1,k_2,\\ldots,k_N\\}$\n\n* Value function is then a vector $V\\in\\mathbb R^N$ with $V[n]$ being the value of having capital $k[n]$\n\n* $V$ then solves the Bellman equation:\n$$\n   V[n] = \\max_{n'}\\ln(A(k[n])^\\alpha - k[n'])+\\beta V[n']\n$$\n\n* RHS is a function $T:\\mathbb R^N \\mapsto \\mathbb R^N\n    * Define it as \n    $$\n    T(V)[n] = \\max_{n'}\\ln(A(k[n])^\\alpha - k[n'])+\\beta V[n']\n    $$\n    \n    * Solution to Bellman equation is vector $V$ such that\n    $$\n        V = T(V)\n    $$ \n\n* Want to: represent $T$ in julia\n\n\n## RBC Bellman Map Code"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "\"\"\"\n   RBCbellmanmap(V,kgrid,A,α,β)\n\nIterates on the bellman equation for the standard neoclassical growth model\n\n# Arguments\n* `V` Vector of values for each capital level in kgrid\n* `kgrid` Vector of capital levels\n* `A` TFP\n* `α` production function is A k^α\n* `β` Time Discount factor\n\"\"\"\nfunction RBCbellmanmap(V,  kgrid,A,α,β)\n    N = length(kgrid)\n    V_out = zeros(N) #new value function\n    n_pol = zeros(Int,N) #policy rule for grid points\n    k_pol = zeros(N) #policy rule for capital\n    obj = zeros(N) #objective to be maximized\n    for n in 1:N #iterate for each initial capital\n        for nprime in 1:N #iterate for choice of capital this period\n            c = A*kgrid[n]^α - kgrid[nprime] #compute consumption\n            if c <= 0\n                obj[nprime] = -Inf #penalty if consumption <0\n            else\n                obj[nprime] = log(c)+β*V[nprime] #otherwise objective from RHS of bellman equation\n            end\n        end\n        V_out[n],n_pol[n] = findmax(obj) #find optimal value and the choice that gives it\n        k_pol[n] = kgrid[n_pol[n]] #record capital policy\n    end\n    return V_out,n_pol,k_pol\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the code"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "N = 50\nA = 1.\nα = 0.33\nβ = 0.95\nkgrid = LinRange(0.01,0.5,N)\nV,n_pol,k_pol = RBCbellmanmap(zeros(N),kgrid,A,α,β )\nprintln(V)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solving RBC Model Code"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "\"\"\"\n    RBCsolve_bellman(V0,kgrid,A,α ,β[,ϵ=1e-6])\n\nSolves the bellman equation by iterating until convergence\n\n# Arguments\n* `V0` Initial vector of values for each capital level in kgrid\n* `kgrid` Vector of capital levels\n* `A` TFP\n* `α` production function is A k^α\n* `β` Time Discount factor\n* `ϵ` Convergence criteria\n\"\"\"\nfunction RBCsolve_bellman(V0,kgrid,A,α,β,ϵ=1e-6)\n    diff = 1.\n    V,n_pol,k_pol = RBCbellmanmap(V0,kgrid,A,α,β)\n    while diff > ϵ\n        V_new,n_pol,k_pol = RBCbellmanmap(V,kgrid,A,α,β)\n        diff = norm(V_new-V,Inf)\n        V = V_new \n    end\n    return V,n_pol,k_pol\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare to Analytical Solution: Value Function"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "V,n_pol,k_pol = RBCsolve_bellman(zeros(N),kgrid,A,α,β )\n\nlayer1 = layer(k->E_infty+F_infty*log(k),.01,.5,color=[\"Analytical\"])\nlayer2 = layer(x=kgrid,y=V,Geom.point,color=[\"Discrete\"],order=1)\nplot(layer1,layer2,Guide.colorkey(title=\"\"),\n    Guide.XLabel(\"Capital\"),Guide.YLabel(\"Value Function\"))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare to Analytical Solution: Policy Rules"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "layer1 = layer(k->(β*F_infty)/(1+β*F_infty)*A*k^α,.01,.5,color=[\"Analytical\"])\nlayer2 = layer(x=kgrid,y=k_pol,Geom.point,color=[\"Discrete\"],order=1)\nplot(layer1,layer2,Guide.colorkey(title=\"\"),\n    Guide.XLabel(\"Capital\"),Guide.YLabel(\"Capital Next Period\"))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Path of Capital\n* One object we will be interested in computing is the path of capital over time\n    * Given some initial $k_0$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "\"\"\"\n    simulate_k(n_0,T,n_pol,kgrid)\n\nSimulates the path of capital given policy rule n_pol and \ninitial capital state kgrid[n_0] for T periods\n\"\"\"\nfunction simulate_k(n_0,T,n_pol,kgrid)\n    k = zeros(T+1) # capital stock\n    n = zeros(Int,T+1) #index of the capital stock\n    n[1] = n_0\n    k[1] = kgrid[n_0]\n    for t in 1:T\n        n[t+1] = n_pol[n[t]] #get the policy rule for the index\n        k[t+1] = kgrid[n[t+1]]\n    end\n\n    return k\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Path of Capital"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "plot(y=simulate_k(1,25,n_pol,kgrid),Guide.xlabel(\"Time\"),Guide.ylabel(\"Capital Stock\"))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Howard Imporvement Algorithm\n## Speed of Value Function Iteration\n\n* Value function iterate can be slow\n* Speed for $N=50$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "N = 50\nkgrid = LinRange(0.01,0.5,N)\n@time RBCsolve_bellman(zeros(N),kgrid,A,α,β);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Speed for $N=500$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "N = 500\nkgrid = LinRange(0.01,0.5,N)\n@time RBCsolve_bellman(zeros(N),kgrid,A,α,β);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Note: does not scale linearly with number of grids.  **Why?**\n\n\n## Howard Improvement Algorithm\n* Performing maximization is slow, much faster to iterate holding policy rules fixed\n* **Idea:** Only update policy rules every $H$ times\n* Otherwise:\n$$\n    V^{j,h+1}[n] = \\ln(Ak[n]^\\alpha - k[{n'}^j[n]])+\\beta V^{j,h}[{n'}^j[n]]\n$$\nwhere ${n'}^j$ is the previous policy\n\n## Howard Improvement Algorithm Code"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "\"\"\"\n    RBCbellmanmap_howard(V,nprime,kgrid,A,α,β)\n\nIterates on the bellman equation for the standard neoclassical growth model using policies nprime,\nrather than computing the optimal policies\n\n# Arguments\n* `V` Vector of values for each capital level in kgrid\n* `n_pol` policy rules k[n_pol[n]] is the capital choice when previous period capital is k[n] \n* `kgrid` Vector of capital levels\n* `A` TFP\n* `α` production function is A k^α\n* `β` Time Discount factor\n\"\"\"\nfunction RBCbellmanmap_howard(V,n_pol,kgrid,A,α,β)\n    N = length(kgrid)\n    V_new = zeros(N)\n    for n in 1:N\n        #use given policy \n        c = A*kgrid[n]^α - kgrid[n_pol[n]]\n        V_new[n] = log(c) + β*V[n_pol[n]]\n    end\n    return V_new\nend;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Howard Improvement Algorithm Code"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "\"\"\"\n    RBCsolve_bellman_howard(V0,H,kgrid,A,α ,β[,ϵ=1e-6])\n\nSolves the bellman equation by iterating until convergence. Uses howard improvement algorithm:\nonly solves for optimal policy every H iteration\n\n# Arguments\n* `V0` Initial vector of values for each capital level in kgrid\n* `H` Controls how frequently optimal policy is solved, H=1 implies every period\n* `kgrid` Vector of capital levels\n* `A` TFP\n* `α` production function is A k^α\n* `β` Time Discount factor\n* `ϵ` Convergence criteria\n\"\"\"\nfunction RBCsolve_bellman_howard(V0,H,kgrid,A,α,β;ϵ=1e-6)\n    diff = 1.\n    V,n_pol,k_pol = RBCbellmanmap(V0,kgrid,A,α,β)\n    #do 5 or so iterations first to allow policys to converge\n    for j in 1:5\n        V_new,n_pol,k_pol = RBCbellmanmap(V,kgrid,A,α,β)\n        V = V_new\n    end\n    #Now apply the Howard Improvement Algorithm\n    while diff > ϵ\n        V_old = V \n        for h in 1:H\n            V_new = RBCbellmanmap_howard(V,n_pol,kgrid,A,α,β)\n            V = V_new\n        end\n        #perform one iteration updating policies\n        V_new,n_pol,k_pol = RBCbellmanmap(V,kgrid,A,α,β)\n        diff = norm(V_new-V_old,Inf)\n        V = V_new\n    end\n    return V,n_pol,k_pol\nend;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How Big a Speed Up N=50"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "N = 50\nkgrid = LinRange(0.01,0.5,N)\n#evaluate once to compile\nRBCsolve_bellman_howard(zeros(N),100,kgrid,A,α,β);\n#Test Timing\n@time RBCsolve_bellman_howard(zeros(N),100,kgrid,A,α,β);\n@time RBCsolve_bellman(zeros(N),kgrid,A,α,β);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How Big a Speed Up N=500"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "N = 500\nkgrid = LinRange(0.01,0.5,N)\n#Test Timing\n@time RBCsolve_bellman_howard(zeros(N),100,kgrid,A,α,β);\n@time RBCsolve_bellman(zeros(N),kgrid,A,α,β);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uncertainty\n\n## Introducing Aggregate Uncertainty\n\n* Add to our generic environment\n\n* Let $\\beta \\in (0,1)$ be a discount factor\n\n* Can choose a sequence of controls $\\{u_t\\}_{t=0}^\\infty$\n\n* Maximize\n$$\n    \\mathbb E\\left[\\sum_{t=0}^\\infty \\beta ^t r(x_{t-1},u_t,\\epsilon_t)\\bigg|\\epsilon_{0}\\right],\n$$\n* subject to $x_{t} = g(x_{t-1},u_t,\\epsilon_t)$ with $x_{-1}\\in \\mathbb R^n$ and $\\epsilon_{0}$ given \n\n* $\\epsilon_t$ is a Markov random variable: $\\epsilon_t \\sim f(\\epsilon_t | \\epsilon_{t-1})$ \n\n## Bellman Equation\n* Let $V(\\epsilon,x)$ be the value of entering with state $x$ and previous shock $\\epsilon$\n\n* This gives the following Bellman Equation\n$$\n    V(\\epsilon,x)= \\max_{u}  r(x,u,\\epsilon) +\\mathbb E[\\beta V(\\epsilon',g(x,u,\\epsilon))|\\epsilon]\n$$\n\n* Want:\n    * Root of this functional equation: $V(\\epsilon,x)$\n    * Optimal Policy:\n    $$\n        h(\\epsilon,x) = \\arg\\max_{u}  r(x,u,\\epsilon) +\\mathbb E[\\beta V(\\epsilon',g(x,u,\\epsilon))|\\epsilon]\n    $$\n\n## Example\n* Let $\\epsilon_t$ follow a finite state Markov process with \n    * Values $\\bar \\epsilon_1,\\ldots, \\bar \\epsilon_n$\n\n    * Transition matrix $\\Pi$ s.t. $\\Pi_{ij}$ is the probability of transitioning $\\epsilon_{t-1} = \\bar \\epsilon_i$ to $\\epsilon_t = \\bar \\epsilon_j$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "#Uncertainty\nA = [0.97,1.03]\n#Transition matrix\nΠ = [0.6 0.4;0.4 0.6];"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stochastic Bellman Code"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "\"\"\"\n    RBCbellmanmap_stochastic(V,kgrid,A,Π,α,β,U)\n\nIterates on the bellman equation for the standard neoclassical growth model\n\n# Arguments\n* `V` Vector of values for each capital level in kgrid\n* `kgrid` Vector of capital levels\n* `A` Vector of TFP values for each state\n* `Π` Transition matrix\n* `α` production function is A k^α\n* `β` Time Discount factor\n\"\"\"\nfunction RBCbellmanmap_stochastic(V,kgrid,A,Π,α,β)\n    N = length(kgrid) #Number of gridpoints of capital\n    S = length(A) #Number of stochastic states\n    V_new = zeros(S,N) #New Value function\n    n_pol = zeros(Int,S,N) #New policy rule for grid points\n    k_pol = zeros(S,N) #New policy rule for capital\n    obj = zeros(N) #objective to be maximized\n    EV = Π*V #precompute expected value for speed\n    for n in 1:N\n        for s in 1:S\n            for nprime in 1:N\n                c = A[s]*kgrid[n]^α - kgrid[nprime] #compute consumption\n                if c <= 0\n                    obj[nprime] = -Inf #punish if c <=0\n                else\n                    obj[nprime] = log(c) + β*EV[s,nprime] #otherwise compute objective\n                end\n            end\n            #find optimal value and policy\n            V_new[s,n],n_pol[s,n] = findmax(obj)\n            k_pol[s,n] = kgrid[n_pol[s,n]]\n        end\n    end\n    return V_new,n_pol,k_pol\nend;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solving The Stochastic Bellman"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "\"\"\"\n    RBCsolve_bellman_stochastic(V0,kgrid,A,α ,β[,ϵ=1e-6])\n\nSolves the bellman equation by iterating until convergence\n\n# Arguments\n* `V0` Initial vector of values for each capital level in kgrid\n* `kgrid` Vector of capital levels\n* `A` Vector of TFP values for each state\n* `Π` Transition matrix\n* `α` production function is A k^α\n* `β` Time Discount factor\n* `ϵ` Convergence criteria\n\"\"\"\nfunction RBCsolve_bellman_stochastic(V0,kgrid,A,Π,α,β,ϵ=1e-6)\n    diff = 1.\n    V,n_pol,k_pol = RBCbellmanmap_stochastic(V0,kgrid,A,Π,α,β)\n    while diff > ϵ\n        V_new,n_pol,k_pol = RBCbellmanmap_stochastic(V,kgrid,A,Π,α,β)\n        diff = norm(V_new-V,Inf)\n        V = V_new\n    end\n    return V,n_pol,k_pol\nend;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Special Case\n* Under log utility we can find the optimal decisions in closed form\n\n* $c(k,A) = (1-\\alpha\\beta)A k^\\alpha$\n\n* $k'(k,A) = \\alpha\\beta A k^\\alpha$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "N = 50\nkgrid = LinRange(0.01,0.5,N)\nV,n_pol,k_pol = RBCsolve_bellman_stochastic(zeros(2,N),kgrid,A,Π,α,β);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy rules"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "layers = []\nfor s in 1:2\n    push!(layers,layer(k->α*β*A[s]*k^α,.01,.5,style(default_color=colorant\"red\")))\n    push!(layers,layer(x=kgrid,y=k_pol[s,:],Geom.point,style(default_color=colorant\"blue\"),order=1))\nend\nplot(layers...,Guide.manual_color_key(\"\",[\"Analytical\",\"Discrete\"],[\"red\",\"blue\"]),\nGuide.XLabel(\"Capital\"),Guide.YLabel(\"Capital Next Period\"))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Path of Capital\n* One object we will be interested in computing is the path of capital over time\n    * Given some initial $k_0$ and $s_0$\n    * Will be stochastic"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using QuantEcon\n\"\"\"\n    simulate_k_stochastic(n_0,T,n_pol,kgrid,Π)\n\nSimulates the path of capital given policy rule n_pol and \ninitial capital state kgrid[n_0] and aggregate state s_0 for T periods\n\"\"\"\nfunction simulate_k_stochastic(n_0,s0,T,n_pol,kgrid,Π)\n    k = zeros(T+1) # capital stock\n    n = zeros(Int,T+1) #index of the capital stock\n    s = simulate_indices(MarkovChain(Π),T;init=s0)\n    n[1] = n_0\n    k[1] = kgrid[n_0]\n    for t in 1:T\n        n[t+1] = n_pol[s[t],n[t]] #get the policy rule for the index\n        k[t+1] = kgrid[n[t+1]]\n    end\n\n    return k\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Path of Capital"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "plot(y=simulate_k_stochastic(1,1,25,n_pol,kgrid,Π),Guide.xlabel(\"Time\"),Guide.ylabel(\"Capital Stock\"))"
      ],
      "metadata": {},
      "execution_count": null
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.6.0"
    },
    "kernelspec": {
      "name": "julia-1.6",
      "display_name": "Julia 1.6.0",
      "language": "julia"
    }
  },
  "nbformat": 4
}
