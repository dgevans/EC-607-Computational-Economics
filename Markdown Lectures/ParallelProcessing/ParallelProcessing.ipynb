{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Parallel Computing\n",
        "<style type=\"text/css\">\n",
        "  .reveal p {\n",
        "    text-align: left;\n",
        "  }\n",
        "  .reveal ul {\n",
        "    display: block;\n",
        "  }\n",
        "  .reveal ol {\n",
        "    display: block;\n",
        "  }\n",
        "</style>\n",
        "# Shared Or Distributed Memory\n",
        "## Two Types of Parallelization\n",
        "* Broadly speaking there are two types of parallelization architectures:\n",
        "\n",
        "    * Shared Memory: all processors live on  the same machine and have access to the same memory\n",
        "   \n",
        "    * Distributed Memory: processors live on different machines and/or have access to their own memory\n",
        "\n",
        "* How you can parallelize code depends on what type of machine(s) you are dealing with\n",
        "    \n",
        "    * Both have their own advantages and disadvantages\n",
        "\n",
        "## Shared Memory\n",
        "* Shared Memory **Advantages**:\n",
        "   \n",
        "    * Don't need to worry about communication\n",
        "   \n",
        "    * Generally only need to put an indicator in front of a for loop or map\n",
        "\n",
        "* Shared Memory **Disadvantages**:\n",
        "   \n",
        "    * Limited in sclare (only so many processors you can put in one machine)\n",
        "   \n",
        "    * Can easily run into race conditions\n",
        "\n",
        "## Distributed Memory\n",
        "* Distributed Memory **Advantages**:\n",
        "   \n",
        "    * Can, in principle, scale arbitrarily large\n",
        "   \n",
        "    * Precise control over communication between processes\n",
        "\n",
        "* Distributed Memory **Disadvantages**:\n",
        "   \n",
        "    * Extra effort on the programmers part\n",
        "\n",
        "# Shared Memory\n",
        "## Threads\n",
        "* There are a lot of different types of shared memory parallel code across languages\n",
        "   \n",
        "    * Open MP (most common)\n",
        "    \n",
        "    * Pthreads\n",
        "    \n",
        "    * etc.\n",
        "\n",
        "* All generally act in the same way\n",
        "\n",
        "* In julia this is implemented using the Threads built-in library\n",
        "\n",
        "* How many threads are active depend on how you started the julia application\n",
        "`julia --threads 8`\n",
        "\n",
        "* You can check how many are active using `nthreads()`"
      ],
      "metadata": {}
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I have 1 active threads\nThis is my id 1\n"
          ]
        }
      ],
      "cell_type": "code",
      "source": [
        "println(\"I have $(Threads.nthreads()) active threads\")\n",
        "println(\"This is my id $(Threads.threadid())\")#Main Thread id is 1"
      ],
      "metadata": {},
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parallelizing A For Loop\n",
        "* Let's start with a simple for loop"
      ],
      "metadata": {}
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.0, 4.0, 9.0, 16.0, 25.0, 36.0, 49.0, 64.0, 81.0, 100.0]\n"
          ]
        }
      ],
      "cell_type": "code",
      "source": [
        "a = zeros(10)\n",
        "for i in 1:10\n",
        "    a[i] = i^2\n",
        "end\n",
        "println(a)"
      ],
      "metadata": {},
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We can easily make this for loop run in parallel using `Threads.@threads`"
      ],
      "metadata": {}
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.0, 4.0, 9.0, 16.0, 25.0, 36.0, 49.0, 64.0, 81.0, 100.0]\n"
          ]
        }
      ],
      "cell_type": "code",
      "source": [
        "a = zeros(10)\n",
        "Threads.@threads for i in 1:10\n",
        "    a[i] = i^2\n",
        "end\n",
        "println(a)"
      ],
      "metadata": {},
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": [
        "* It's just that simple\n",
        "\n",
        "## Digging deeper into @threads\n",
        "* Let's dig a little deeper into that macro: what's going on under the hood"
      ],
      "metadata": {}
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.0, 4.0, 9.0, 16.0, 25.0, 36.0, 49.0, 64.0, 81.0, 100.0]\n[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
          ]
        }
      ],
      "cell_type": "code",
      "source": [
        "a = zeros(10)\n",
        "id = zeros(10)\n",
        "Threads.@threads for i in 1:10\n",
        "    a[i] = i^2\n",
        "    id[i] = Threads.threadid()\n",
        "end\n",
        "println(a)\n",
        "println(id)"
      ],
      "metadata": {},
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "source": [
        "* There are 8 threads\n",
        "\n",
        "* Each thread is assigned a portion of the \"work\"\n",
        "\n",
        "* Note: there are 10 tasks so threads 1 and 2 do twice the work\n",
        "    \n",
        "    * Therefore won't get a full 8x speed up\n",
        "    \n",
        "    * Best case scenario is a 4x speed up\n",
        "    \n",
        "    * In reality may run slower because there's a small overhead to each paralle for loop\n",
        "\n",
        "\n",
        "## Race conditions\n",
        "* In our previous example all the threads are modifying the same vector $a$\n",
        "\n",
        "* This is a great advantage: \n",
        "\n",
        "    * Don't need to worry about how to share memomry across processes\n",
        "\n",
        "* BUT: what happens if two threads try and modify the same element of $a$\n",
        "    \n",
        "* Can run into a race condition:\n",
        "\n",
        "    * Results may depend on which process modified $a$ first (hence race)\n",
        "\n",
        "    * Will lead to **unstable** and **incorrect** output\n",
        "\n",
        "## A Simple example\n",
        "* Let's start with a simple example where we add 1 to a vecto 10,000 times"
      ],
      "metadata": {}
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10000.0, 10000.0, 10000.0]\n"
          ]
        }
      ],
      "cell_type": "code",
      "source": [
        "acc = zeros(3)\n",
        "for i in 1:10_000\n",
        "    acc .+= 1\n",
        "end\n",
        "println(acc)"
      ],
      "metadata": {},
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Produces exactly what we would expect\n",
        "\n",
        "    * What happens if we parallelize it?"
      ],
      "metadata": {}
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10000.0, 10000.0, 10000.0]\n"
          ]
        }
      ],
      "cell_type": "code",
      "source": [
        "acc = zeros(3)\n",
        "Threads.@threads for i in 1:10_000\n",
        "    acc .+= 1\n",
        "end\n",
        "println(acc)"
      ],
      "metadata": {},
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Huh?  What happened here?\n",
        "\n",
        "## Digging Into The Race condition\n",
        "\n",
        "* All that is going on can be understood by seeing what `acc .+= 1` is doing\n",
        "\n",
        "* Essentially `acc .+= 1`:\n",
        "    \n",
        "    1. Takes a copy of an element `acc`\n",
        "    \n",
        "    2. Adds 1 to that copy\n",
        "    \n",
        "    3. Overwrites that element of `acc` with its new value\n",
        "\n",
        "* Now suppose that process 1 starts tasks 1,2,3\n",
        "    \n",
        "    * While process 1 is on task 2 process 2 starts task 1,2,3\n",
        "\n",
        "    * The results of process 1 will be overwritten by process 2\n",
        "\n",
        "    * That sum won't be preformed\n",
        "\n",
        "* Generally speaking: make sure that all process are working on their own blocks of memory\n",
        "\n",
        "    * Usually this is a natural outcome of our code\n",
        "\n",
        "# Shared Memory Examples\n",
        "\n",
        "## Example: Monte Carlo\n",
        "\n",
        "* Take a very simple monte carlo example\n",
        "\n",
        "* Let $x_t$ follow an AR(1)\n",
        "\n",
        "* Want a sample of the distribution of $x_t$ after $1000$ periods\n",
        "    * A sample of size $100,000$\n",
        "\n",
        "* We'll need to draw $100,000$ draws form this AR(1) process\n",
        "    * Ideal for parallelization\n",
        "\n",
        "## AR(1) Code"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Parameters\n",
        "mutable struct AR1\n",
        "    μ::Float64 #Mean of the AR(1)\n",
        "    ρ::Float64 #persistence of the AR(1)\n",
        "    σ::Float64 #standard deviaiton of the AR(1)\n",
        "end\n",
        "\"\"\"\n",
        "   simulateAR1(ar,x0,T)\n",
        "\n",
        "Simulates an AR(1) ar for T periods with initial value x0\n",
        "\"\"\"\n",
        "function simulateAR1(ar,x0,T)\n",
        "    @unpack σ,μ,ρ = ar #note order doesn't matter\n",
        "    x = zeros(T+1)# initialize\n",
        "    x[1] = x0\n",
        "    for t in 1:T\n",
        "        x[t+1] = (1-ρ)*μ + ρ*x[t] + σ*randn()\n",
        "    end\n",
        "    return x[2:end]\n",
        "end;"
      ],
      "metadata": {},
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How Does It Behave Serially"
      ],
      "metadata": {}
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1.147890 seconds (200.00 k allocations: 1.521 GiB, 16.17% gc time)\n"
          ]
        }
      ],
      "cell_type": "code",
      "source": [
        "#get a sample of 100_000 of x after 100 periods\n",
        "\"\"\"\n",
        "    draw_endofT_sample(ar,N,T)\n",
        "\n",
        "Draws N values of x after T periods where x follows AR1 with x[1] = 0.\n",
        "\"\"\"\n",
        "function draw_endofT_sample(ar,N,T)\n",
        "    sample = zeros(N)\n",
        "    for i in 1:N\n",
        "        sample[i] = simulateAR1(ar,0.,T)[end]\n",
        "    end\n",
        "    return sample\n",
        "end\n",
        "\n",
        "ar = AR1(1.,.99,0.01)\n",
        "sample = draw_endofT_sample(ar,100_000,1000)\n",
        "@time draw_endofT_sample(ar,100_000,1000);"
      ],
      "metadata": {},
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parallelizing is Easy!"
      ],
      "metadata": {}
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0.999745 seconds (200.01 k allocations: 1.521 GiB, 16.72% gc time)\n"
          ]
        }
      ],
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    draw_endofT_sample(ar,N,T)\n",
        "\n",
        "Draws N values of x after T periods where x follows AR1 with x[1] = 0.\n",
        "\"\"\"\n",
        "function draw_endofT_sample_parallel(ar,N,T)\n",
        "    sample = zeros(N)\n",
        "    Threads.@threads for i in 1:N\n",
        "        sample[i] = simulateAR1(ar,0.,T)[end]\n",
        "    end\n",
        "    return sample\n",
        "end\n",
        "\n",
        "sample = draw_endofT_sample_parallel(ar,100_000,1000)\n",
        "@time draw_endofT_sample_parallel(ar,100_000,1000);"
      ],
      "metadata": {},
      "execution_count": 9
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Note a lot of the time is spent in garbage collection\n",
        "    * i.e. allocating and deallocating memory\n",
        "    * can we speed this up?\n",
        "\n",
        "## Write More Efficient Serial Code\n",
        "* Don't need to save entire AR(1) process \n",
        "    * Just current value when simulating"
      ],
      "metadata": {}
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0.553427 seconds (2 allocations: 781.328 KiB)\n"
          ]
        }
      ],
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "   simulateAR1final(ar,x0,T)\n",
        "\n",
        "Simulates an AR(1) ar for T periods with initial value x0.\n",
        "Returns the final period value\n",
        "\"\"\"\n",
        "function simulateAR1final(ar,x,T)\n",
        "    @unpack σ,μ,ρ = ar #note order doesn't matter\n",
        "    for t in 1:T\n",
        "        x = (1-ρ)*μ + ρ*x + σ*randn()\n",
        "    end\n",
        "    return x\n",
        "end\n",
        "\n",
        "\"\"\"\n",
        "    draw_endofT_sample(ar,N,T)\n",
        "\n",
        "Draws N values of x after T periods where x follows AR1 with x[1] = 0.\n",
        "\"\"\"\n",
        "function draw_endofT_sample(ar,N,T)\n",
        "    sample = zeros(N)\n",
        "    for i in 1:N\n",
        "        sample[i] = simulateAR1final(ar,0.,T)\n",
        "    end\n",
        "    return sample\n",
        "end\n",
        "sample = draw_endofT_sample(ar,100_000,1000)\n",
        "@time draw_endofT_sample(ar,100_000,1000);"
      ],
      "metadata": {},
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speedup Carries Over to Parallel Code"
      ],
      "metadata": {}
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0.545930 seconds (10 allocations: 781.844 KiB)\n"
          ]
        }
      ],
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    draw_endofT_sample(ar,N,T)\n",
        "\n",
        "Draws N values of x after T periods where x follows AR1 with x[1] = 0.\n",
        "\"\"\"\n",
        "function draw_endofT_sample_parallel(ar,N,T)\n",
        "    sample = zeros(N)\n",
        "    Threads.@threads for i in 1:N\n",
        "        sample[i] = simulateAR1final(ar,0.,T)\n",
        "    end\n",
        "    return sample\n",
        "end\n",
        "sample = draw_endofT_sample_parallel(ar,100_000,1000)\n",
        "@time draw_endofT_sample_parallel(ar,100_000,1000);"
      ],
      "metadata": {},
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Value Function Iteration\n",
        "* Another easily parallelizable algorith is value function iteration\n",
        "\n",
        "* Need to evaluate Bellman Map at all of the interpolation gridpoints\n",
        "    * Can be executed in parallel\n",
        "\n",
        "* Let's see this applied to our code soling the RBC model\n",
        "\n",
        "## RBC Model"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using BasisMatrices,Optim,LinearAlgebra\n",
        "@with_kw mutable struct NCParameters\n",
        "    A::Float64 = 1.\n",
        "    α::Float64 = 0.3\n",
        "    β::Float64 = 0.96\n",
        "    kgrid::Vector{Float64} = LinRange(0.05,0.5,20)\n",
        "    spline_order::Int = 3\n",
        "end;"
      ],
      "metadata": {},
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimal Policy"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    optimalpolicy(para::NCParameters,Vprime,k)\n",
        "\n",
        "Computes optimal policy using continuation value function V and current capital\n",
        "level k given parameters in para.\n",
        "\"\"\"\n",
        "function optimalpolicy(para::NCParameters,Vprime,k)\n",
        "    @unpack A,α,β,kgrid = para\n",
        "    k_bounds = [kgrid[1],kgrid[end]]\n",
        "    f_objective(kprime) = -( log(A*k^α-kprime)+β*Vprime(kprime) ) #stores objective as function\n",
        "    k_max = min(A*k^α-.001,k_bounds[2]) #Can't have negative consumptions\n",
        "    result = optimize(f_objective,k_bounds[1],k_max)\n",
        "    return (kprime = result.minimizer,V=-result.minimum) #using named tuples \n",
        "end;"
      ],
      "metadata": {},
      "execution_count": 13
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solving the Bellman Equation"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    bellmanmap(Vprime,para::NCParameters)\n",
        "\n",
        "Apply the bellman map given continuation value function Vprime\n",
        "\"\"\"\n",
        "function bellmanmap(para::NCParameters,Vprime::Interpoland)\n",
        "    kbasis = Vprime.basis\n",
        "    #sometimes it's helpful to tell julia what type a variable is\n",
        "    knodes = nodes(kbasis)[1]::Vector{Float64}\n",
        "    V = map(k->optimalpolicy(para,Vprime,k).V,knodes)\n",
        "    return Interpoland(kbasis,V)\n",
        "end\n",
        "\n",
        "\"\"\"\n",
        "    solvebellman(para::NCParameters,V0::Interpoland)\n",
        "\n",
        "Solves the bellman equation for a given V0\n",
        "\"\"\"\n",
        "function solvebellman(para::NCParameters,V0::Interpoland)\n",
        "    diff = 1\n",
        "    #Iterate of Bellman Map until difference in coefficients goes to zero\n",
        "    while diff > 1e-6\n",
        "        V = bellmanmap(para,V0)\n",
        "        diff = norm(V.coefs-V0.coefs,Inf)\n",
        "        V0 = V\n",
        "    end\n",
        "    kbasis = V0.basis\n",
        "    knodes = nodes(kbasis)[1]::Vector{Float64}\n",
        "    #remember optimalpolicy also returns the argmax\n",
        "    kprime = map(k->optimalpolicy(para,V0,k).kprime,knodes)\n",
        "    #Now get policies\n",
        "    return Interpoland(kbasis,kprime),V0\n",
        "end;"
      ],
      "metadata": {},
      "execution_count": 14
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Serial Benchmark"
      ],
      "metadata": {}
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1.638470 seconds (18.75 M allocations: 1.866 GiB, 16.64% gc time)\n"
          ]
        }
      ],
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    getV0(para::NCParameters)\n",
        "\n",
        "Initializes V0(k) = 0 using the kgrid of para\n",
        "\"\"\"\n",
        "function getV0(para::NCParameters)\n",
        "    @unpack kgrid,spline_order = para\n",
        "\n",
        "    kbasis = Basis(SplineParams(kgrid,0,spline_order))\n",
        "\n",
        "    return Interpoland(kbasis,k->0 .*k)\n",
        "end\n",
        "para = NCParameters()\n",
        "para.kgrid = LinRange(0.05,0.5,100)\n",
        "kprime,V = solvebellman(para,getV0(para))\n",
        "@time solvebellman(para,getV0(para));"
      ],
      "metadata": {},
      "execution_count": 15
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How To Parallelize?\n",
        "*  `V = map(k->optimalpolicy(para,Vprime,k).V,knodes)` evaluates the optimal policy\n",
        "    * at every point in the state space\n",
        "\n",
        "* Want: to do this in parallel\n",
        "    * for a given `Vprime` all the evaluations are independent\n",
        "\n",
        "* The `map` function is just a for loop so we can use Threads\n",
        "\n",
        "* The library `ThreadTools` provides us with a convenient way of doing this\n",
        "    * Replace `map` with `tmap`\n",
        "\n",
        "\n",
        "## Parallelizing Bellman Map"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using ThreadTools\n",
        "\"\"\"\n",
        "    bellmanmap_parallel(Vprime,para::NCParameters)\n",
        "\n",
        "Apply the bellman map given continuation value function Vprime\n",
        "\"\"\"\n",
        "function bellmanmap_parallel(para::NCParameters,Vprime::Interpoland)\n",
        "    kbasis = Vprime.basis\n",
        "    #sometimes it's helpful to tell julia what type a variable is\n",
        "    knodes = nodes(kbasis)[1]::Vector{Float64}\n",
        "    V = tmap(k->optimalpolicy(para,Vprime,k).V,knodes)\n",
        "    return Interpoland(kbasis,V)\n",
        "end\n",
        "\n",
        "\"\"\"\n",
        "    solvebellman_parallel(para::NCParameters,V0::Interpoland)\n",
        "\n",
        "Solves the bellman equation for a given V0\n",
        "\"\"\"\n",
        "function solvebellman_parallel(para::NCParameters,V0::Interpoland)\n",
        "    diff = 1\n",
        "    #Iterate of Bellman Map until difference in coefficients goes to zero\n",
        "    while diff > 1e-6\n",
        "        V = bellmanmap_parallel(para,V0)\n",
        "        diff = norm(V.coefs-V0.coefs,Inf)\n",
        "        V0 = V\n",
        "    end\n",
        "    kbasis = V0.basis\n",
        "    knodes = nodes(kbasis)[1]::Vector{Float64}\n",
        "    #remember optimalpolicy also returns the argmax\n",
        "    kprime = tmap(k->optimalpolicy(para,V0,k).kprime,knodes)\n",
        "    #Now get policies\n",
        "    return Interpoland(kbasis,kprime),V0\n",
        "end;"
      ],
      "metadata": {},
      "execution_count": 16
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Total Speedup"
      ],
      "metadata": {}
    },
    {
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1.634650 seconds (18.96 M allocations: 1.880 GiB, 11.90% gc time)\n"
          ]
        }
      ],
      "cell_type": "code",
      "source": [
        "kprime,V = solvebellman_parallel(para,getV0(para))\n",
        "@time solvebellman_parallel(para,getV0(para));"
      ],
      "metadata": {},
      "execution_count": 17
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distributed Memory\n",
        "## MPI\n",
        "* Lot's of different tools to do distributed memory computations\n",
        "\n",
        "* Most ubiquitous is MPI (Message Passign Interface)\n",
        "\n",
        "* Julia has their own toolset \n",
        "    * But I've found it less convenient for economics problems\n",
        "\n",
        "* We'll learn the MPI library \n",
        "    * Using the Julia cluster manager\n",
        "\n",
        "## MPI Cluster Manager\n",
        "* The following spawns 4 worker processes\n",
        "    * They can communicate together using MPI\n",
        "    \n",
        "* Do not need to be on the same machine\n",
        "    * Do not have access to the same memory"
      ],
      "metadata": {}
    },
    {
      "outputs": [
        {
          "output_type": "error",
          "ename": "LoadError",
          "evalue": "UndefVarError: MPIManager not defined",
          "traceback": [
            "UndefVarError: MPIManager not defined",
            "",
            "Stacktrace:",
            " [1] top-level scope",
            "   @ In[18]:2",
            " [2] eval",
            "   @ ./boot.jl:360 [inlined]",
            " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
            "   @ Base ./loading.jl:1094"
          ]
        }
      ],
      "cell_type": "code",
      "source": [
        "using MPIClusterManagers,Distributed\n",
        "manager = MPIManager(np=4)\n",
        "\n",
        "#spawns the additional processes\n",
        "addprocs(manager)"
      ],
      "metadata": {},
      "execution_count": 18
    },
    {
      "cell_type": "markdown",
      "source": [
        "## @mpi_do\n",
        "* MPIClusterManagers has a convenient macro `@mpi_do`\n",
        "\n",
        "* It tells all the worker nodes to run the exact same set of code"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@mpi_do manager begin #manager represents the cluster we are working with\n",
        "    a = 2\n",
        "    b = 3\n",
        "    println(\"For a^b I get $(a^b)\")\n",
        "end;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Communicator Objective\n",
        "* A communicator object defines how processes communicate\n",
        "    * Specifies which processes \n",
        "    * Gives each process a specific rank\n",
        "\n",
        "* We'll work with the global communicator\n",
        "    * i.e. all the processes\n",
        "    * can be accessed via `MPI.COMM_WORLD`\n",
        "\n",
        "* Some simple functions can get the size of the communicator\n",
        "    * and the individual processes rank (counts from 0)"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@mpi_do manager begin\n",
        "    using MPI\n",
        "    comm=MPI.COMM_WORLD #comm defines who is communicating, comm_world is all the processes\n",
        "    println(\"Hello world, I am $(MPI.Comm_rank(comm)) of $(MPI.Comm_size(comm))\")\n",
        "end"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some Basic Communication\n",
        "* The heart of MPI is how processes communicate\n",
        "    * can be one to one, many to one, one to many, or many to many\n",
        "    * we'll focus on a few which are particularly useful\n",
        "\n",
        "* `Bcast!`\n",
        "    * Root process (usually rank 0) sends data to all other processes\n",
        "\n",
        "* `Gather`\n",
        "    * Root process gather's data from all other processes \n",
        "\n",
        "* `Allgather`  \n",
        "    * Same as gather but all processes receive the same data\n",
        "\n",
        "## Bcast Example"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@mpi_do manager begin\n",
        "    data = rand(1)\n",
        "    println(\"Hi, I'm rank $(MPI.Comm_rank(comm)) and my data is $data\")\n",
        "    MPI.Bcast!(data,0,comm) #Bcast data using root 0 and COMM_WORLD communicator\n",
        "    println(\"Hi, I'm rank $(MPI.Comm_rank(comm)) and my data is now  $data\")\n",
        "end"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gather Example"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@mpi_do manager begin\n",
        "    data = rand(1)\n",
        "    println(\"Hi, I'm rank $(MPI.Comm_rank(comm)) and my data is $data\")\n",
        "    gathered_data = MPI.Gather(data,0,comm) #Bcast data using root 0 and COMM_WORLD communicator\n",
        "    println(\"Hi, I'm rank $(MPI.Comm_rank(comm)) here is the combined data  $gathered_data\")\n",
        "end"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Allgather Example"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@mpi_do manager begin\n",
        "    data = rand(1)\n",
        "    println(\"Hi, I'm rank $(MPI.Comm_rank(comm)) and my data is $data\")\n",
        "    gathered_data = MPI.Allgather(data,comm) #Bcast data using root 0 and COMM_WORLD communicator\n",
        "    println(\"Hi, I'm rank $(MPI.Comm_rank(comm)) here is the combined data  $gathered_data\")\n",
        "end"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MPI Examples\n",
        "\n",
        "## AR(1) Example\n",
        "* Go back to the problem of sampling ergodic distribution\n",
        "\n",
        "* If we want to sample from $100,000$ points with two processes\n",
        "    * Have each draw $50,000$ and the gather the data\n",
        "\n",
        "* Very little of the code needs to change\n",
        "\n",
        "* Just need to make sure that each process has AR(1) code on it"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@mpi_do manager begin\n",
        "    using Parameters\n",
        "end"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@mpi_do manager begin\n",
        "    using Parameters\n",
        "    mutable struct AR1\n",
        "        μ::Float64 #Mean of the AR(1)\n",
        "        ρ::Float64 #persistence of the AR(1)\n",
        "        σ::Float64 #standard deviaiton of the AR(1)\n",
        "    end\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    simulateAR1(ar,x0,T)\n",
        "\n",
        "    Simulates an AR(1) ar for T periods with initial value x0\n",
        "    \"\"\"\n",
        "    function simulateAR1(ar,x0,T)\n",
        "        @unpack σ,μ,ρ = ar #note order doesn't matter\n",
        "        x = zeros(T+1)# initialize\n",
        "        x[1] = x0\n",
        "        for t in 1:T\n",
        "            x[t+1] = (1-ρ)*μ + ρ*x[t] + σ*randn()\n",
        "        end\n",
        "        return x[2:end]\n",
        "    end\n",
        "\n",
        "\n",
        "    #get a sample of 100_000 of x after 100 periods\n",
        "    \"\"\"\n",
        "        draw_endofT_sample(ar,N,T)\n",
        "\n",
        "    Draws N values of x after T periods where x follows AR1 with x[1] = 0.\n",
        "    \"\"\"\n",
        "    function draw_endofT_sample(ar,N,T)\n",
        "        sample = zeros(N)\n",
        "        for i in 1:N\n",
        "            sample[i] = simulateAR1(ar,0.,T)[end]\n",
        "        end\n",
        "        return sample\n",
        "    end\n",
        "end;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constructing the sample"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@mpi_do manager begin \n",
        "    ar = AR1(1.,.99,0.01)\n",
        "    sample = draw_endofT_sample(ar,50000,1000) #note each process draws its own sample \n",
        "    #How to get them all together\n",
        "end;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Use Allgather (or gather) to get all the sample in one process"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@mpi_do manager begin\n",
        "    samples = MPI.Allgather(sample,comm) #All the processes gather data\n",
        "    println(sum(samples))\n",
        "end"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Value Function Iteration\n",
        "* Using value function iteration requires a little bit more planning\n",
        "\n",
        "* Want to parallelize applying bellman map over state space gridpoints\n",
        "    * This is a vector of length $N$\n",
        "\n",
        "* Each process should only be computing values at subset of gridpoints \n",
        "    * Combined with others at the end\n",
        "\n",
        "* Easiest to do then $S$ (# of processes) divides  $N$ evenly: $N=S\\times K$\n",
        "    * Rank 0 process takes first $K$ elements\n",
        "    * Rank 1 process takes elements $K+1$ to $2K$ \n",
        "    * Rank 2 process takes elements $2K+1$ to $3K$\n",
        "    * ...\n",
        "    * Rank S process takes elements $(S-1)K+1$ to $SK$\n",
        "\n",
        "## Common Code"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@mpi_do manager begin\n",
        "    using BasisMatrices,Optim,LinearAlgebra\n",
        "    @with_kw mutable struct NCParameters\n",
        "        A::Float64 = 1.\n",
        "        α::Float64 = 0.3\n",
        "        β::Float64 = 0.96\n",
        "        kgrid::Vector{Float64} = LinRange(0.05,0.5,20)\n",
        "        spline_order::Int = 3\n",
        "    end\n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "        optimalpolicy(para::NCParameters,Vprime,k)\n",
        "\n",
        "    Computes optimal policy using continuation value function V and current capital\n",
        "    level k given parameters in para.\n",
        "    \"\"\"\n",
        "    function optimalpolicy(para::NCParameters,Vprime,k)\n",
        "        @unpack A,α,β,kgrid = para\n",
        "        k_bounds = [kgrid[1],kgrid[end]]\n",
        "        f_objective(kprime) = -( log(A*k^α-kprime)+β*Vprime(kprime) ) #stores objective as function\n",
        "        k_max = min(A*k^α-.001,k_bounds[2]) #Can't have negative consumptions\n",
        "        result = optimize(f_objective,k_bounds[1],k_max)\n",
        "        return (kprime = result.minimizer,V=-result.minimum) #using named tuples \n",
        "    end\n",
        "\n",
        "    \"\"\"\n",
        "        getV0(para::NCParameters)\n",
        "\n",
        "    Initializes V0(k) = 0 using the kgrid of para\n",
        "    \"\"\"\n",
        "    function getV0(para::NCParameters)\n",
        "        @unpack kgrid,spline_order = para\n",
        "\n",
        "        kbasis = Basis(SplineParams(kgrid,0,spline_order))\n",
        "\n",
        "        return Interpoland(kbasis,k->0 .*k)\n",
        "    end\n",
        "end;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MPI Bellman Map Code"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@mpi_do manager begin\n",
        "    \"\"\"\n",
        "    bellmanmap_mpi(Vprime,para::NCParameters)\n",
        "\n",
        "    Apply the bellman map given continuation value function Vprime\n",
        "    \"\"\"\n",
        "    function bellmanmap_mpi(para::NCParameters,Vprime::Interpoland)\n",
        "        comm = MPI.COMM_WORLD\n",
        "        r,s = MPI.Comm_rank(comm),MPI.Comm_size(comm) #get size of rank of mpi\n",
        "        kbasis = Vprime.basis\n",
        "        #sometimes it's helpful to tell julia what type a variable is\n",
        "        knodes = nodes(kbasis)[1]::Vector{Float64}\n",
        "        Nk = Int(length(knodes)/s) #note will spit out error if not equally divisible\n",
        "        mynodes = knodes[1+r*Nk:(r+1)*Nk]#select a range of size Nk from all the nodes\n",
        "\n",
        "        my_V = map(k->optimalpolicy(para,Vprime,k).V,mynodes)#only compute value function on my nodes\n",
        "\n",
        "        #Gather at the end\n",
        "        V = MPI.Allgather(my_V,comm)#will be a vector for each process \n",
        "        return Interpoland(kbasis,V) #each process constructs own value function\n",
        "    end\n",
        "end;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MPI Solve Bellman Code"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@mpi_do manager begin    \n",
        "    \"\"\"\n",
        "        solvebellman_mpi(para::NCParameters,V0::Interpoland)\n",
        "\n",
        "    Solves the bellman equation for a given V0\n",
        "    \"\"\"\n",
        "    function solvebellman_mpi(para::NCParameters,V0::Interpoland)\n",
        "        comm = MPI.COMM_WORLD\n",
        "        r,s = MPI.Comm_rank(comm),MPI.Comm_size(comm) #get size of rank of mpi\n",
        "        diff = 1\n",
        "        #Iterate of Bellman Map until difference in coefficients goes to zero\n",
        "        while diff > 1e-6\n",
        "            V = bellmanmap_mpi(para,V0)\n",
        "            diff = norm(V.coefs-V0.coefs,Inf)\n",
        "            V0 = V\n",
        "        end\n",
        "        kbasis = V0.basis\n",
        "        knodes = nodes(kbasis)[1]::Vector{Float64}\n",
        "        Nk = Int(length(knodes)/s) #note will spit out error if not equally divisible\n",
        "        mynodes = knodes[1+r*Nk:(r+1)*Nk]#select a range of size Nk from all the nodes\n",
        "\n",
        "        #remember optimalpolicy also returns the argmax\n",
        "        my_kprime = map(k->optimalpolicy(para,V0,k).kprime,mynodes)\n",
        "        kprime = MPI.Allgather(my_kprime,comm)#will be a vector for each process \n",
        "        #Now get policies\n",
        "        return Interpoland(kbasis,kprime),V0\n",
        "    end\n",
        "end"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the Parallel Code"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@mpi_do manager begin\n",
        "    #Note need to call solvebellman on all processes so they work in \n",
        "    #parallel\n",
        "    para = NCParameters()\n",
        "    para.kgrid = LinRange(0.05,0.5,102) #102 insures 104 nodes with cubic interpolation\n",
        "    kprime,V = solvebellman_mpi(para,getV0(para))\n",
        "end\n",
        "@time @mpi_do manager begin\n",
        "    solvebellman_mpi(para,getV0(para))\n",
        "end"
      ],
      "metadata": {},
      "execution_count": null
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.6.0"
    },
    "kernelspec": {
      "name": "julia-1.6",
      "display_name": "Julia 1.6.0",
      "language": "julia"
    }
  },
  "nbformat": 4
}