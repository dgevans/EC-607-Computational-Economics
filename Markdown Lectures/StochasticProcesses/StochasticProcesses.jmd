# Stochastic Processes
<style type="text/css">
  .reveal p {
    text-align: left;
  }
  .reveal ul {
    display: block;
  }
  .reveal ol {
    display: block;
  }
</style>
## Introduction
* Stochastic processes are the foundation of economic models
    * Capture risk individual agents face

    * Aggregate risk

    * Estimation etc.

* We'll study how to construct and simulate on a computer
    
    * Introduce some basic programming constructs

* Start with simple random numbers

## How To Generate Random numbers

* Generating random number is as simple as calling `rand()`

* Generates a number in $[0,1]$ uniformly distributed
```julia
rand()
```
* Can even choose between elements of a set
```julia
rand([10,20])
```

* If you give it dimensions it will give a vector or matrix of random variables
```julia
rand(2,2)
```

## Constructing Any Random Variable

* Note any random variable can be constructed from the uniform on $[0,1]$

* Let x be a random variable on $\mathbb R$ 
    * let $F(x)$ be its CDF

* If $\xi$ is a r.v. uniform on $[0,1]$ 
    * then $F^{-1}(\xi)$ has the same distribution as $x$

* We can draw pretty much any type of random variable we like
    * `randn` is how we draw the standard normal

## A Simple Monte Carlo

* Let's say we wanted to flip a 100 times how would we do that 

* Start simple: flip a coin 1 time and record 1 as heads 0 as tails
```julia
flip = rand() < 0.5
```

* To do 100 times let's use a for loop and a vector
```julia
flips = zeros(100) #Where to store the flips
r = rand(100) #A vector of 100 uniform random numbers on [0,1]
for i in 1:100
    flips[i] = r[i] < 0.5 #check to see if the i th flip is heads
end
println(flips)
```

## An Even Simpler Way
* Often times we want to vectorize an operation
    * works in place of a for loop

* Using `.` before an operation means apply this to all elements
    * 'broadcasts' across dimensions (we'll look at this later)

* Often very efficient (doesn't create a bunch of new variables)
    * because we work with a lot of vectors we'll do this a lot

* For example our coin flipping exercise becomes
```julia
flips = rand(100) .< 0.5 #note the . won't work without it
println(flips)
```

## Functions
* Let's create a function to do the flipping for us
```julia
"""
    flipNcoins(N,p=0.5)

Flips N coins with probability p of returning 1 (heads)
"""
function flipNcoins(N,p=0.5)
    return rand(N) .< p
end
```
* Check that it works
```julia
println(flipNcoins(15))
```
* Can change weighting of the coins
```julia
println(flipNcoins(15,1.))
```

# The Distribution Package
* The distribution package contains the ability to draw from a lot of distributions
```julia
using Distributions
dist  = LogNormal(0,1)
```
* Has lots of helpful functions
```julia
println(mean(dist))
println(std(dist))
println(quantile(dist,0.5))
println(cdf(dist,1.))
println(pdf(dist,1.))
println(rand(dist))
```

# Some Monte Carlo
## Fliping 50 coins
* Let's flip 50 coins what would be the mean number of heads
```julia
using Statistics
mean(flipNcoins(50))
```
* For an unfair coin
```julia
mean(flipNcoins(50,0.3))
```
## Plotting 
* The next few slides are going to generate plots

* I'm going to use Gadfly as my plotting library
    * Uses language of graphics which makes it really convenient
```julia
using Gadfly
```

* I'm going to use some default values to make the pictures look better (usually I'll hide these)
```julia 
set_default_plot_size(12inch, 6inch) #set size
#make lines and points thicker and bigger labels
Gadfly.push_theme(Theme(major_label_font_size=20pt,minor_label_font_size=14pt,key_label_font_size=16pt,
                        line_width=2pt,point_size=3pt))
```

## Some More Monte Carlo 
* How does the average number of coins behave with the number of tosses $N$?
```julia
meantosses = [mean(flipNcoins(N,0.5)) for N in 1:100]; #This is called a list compression
```
* Can plot to visualize
```julia fig_width=10; fig_height=5
plot(x=1:100,y=meantosses,Guide.XLabel("Tosses"),Guide.YLabel("Average # Heads"))
```

## Some More Monte Carlo
* Can do it for eaven longer simulation to make sure it settles down
```julia fig_width=10; fig_height=5
meantosses = [mean(flipNcoins(N,0.5)) for N in 5:1000]; #This is called a list compression
stdtosses = [std(flipNcoins(N,0.5)) for N in 5:1000]; #This is called a list compression
hstack(plot(x=5:1000,y=meantosses,Geom.line,Guide.XLabel("Tosses"),Guide.YLabel("Average # Heads")),
       plot(x=5:1000,y=stdtosses,Geom.line,Guide.XLabel("Tosses"),Guide.YLabel("STD # Heads"))) 
```

## What Does the Distribution Look Like
* We can get an idea for what the distribution heads looks like for 15 tosses
* And compare to truth (Binomial)
```julia fig_width=10; fig_height=5
numheads = [sum(flipNcoins(15,0.5)) for k in 1:100_000]
plot(layer(x=numheads,Geom.histogram(density=true,bincount=15)),
     layer(x=0:15,y=pdf.(Binomial(15,0.5),0:15),color=[colorant"red"],Geom.point,order=1)
    ,Guide.XLabel("# of Heads"),Guide.YLabel("Probability"))
```

## Pseudo Random variables
* One important thing to know is that all random variables on a computer are not really random 
    * They are actually a deterministic sequence
    * But behave random 
    * Depends on what seed is set.  We can change this with `Random.seed!()`

* The following will print `0.5627138851056968`
```julia
using Random
Random.seed!(12345) #can put any integer here
rand()
```
* Will behave the same way again
```julia
Random.seed!(12345) #can put any integer here
rand()
```

# Continuous Processes

## An AR(1)
* Let's model and AR1 processes
$$
    x_t =(1-\rho)μ + \rho x_{t-1} + \epsilon_t 
$$
where $\epsilon_t\sim\mathcal N(0,\sigma)$

* Depends on two key parameters: $\rho$ and $\sigma$

* Can easily write a function to simulate
```julia
"""
   simulateAR1(ρ,σ,T)

Simulates an AR(1) with mean μ, persistence ρ, and standard deviation σ for 
T periods with initial value x0
"""
function simulateAR1(μ,ρ,σ,x0,T)
    x = zeros(T+1)# initialize
    x[1] = x0
    for t in 1:T
        x[t+1] = (1-ρ)*μ + ρ*x[t] + σ*randn()
    end
    return x[2:end]
end
```

## Plotting and An AR1

* We can see what an AR(1) looks like by plotting an example path
```julia
plot(x=1:100,y=simulateAR1(0.,0.8,1.,0,100),Geom.line,Guide.XLabel("Time"),Guide.YLabel("AR(1)"))
```

## Structs  
* Many times in julia (and programming) it's construct an Type that represents an object
* For example, our AR1 is represented by three variables: $\rho,\mu,$ and $\sigma$
* We can make an mutable struct which represents this AR(1)
```julia results="hidden"
mutable struct AR1
    μ::Float64 #Mean of the AR(1)
    ρ::Float64 #persistence of the AR(1)
    σ::Float64 #standard deviaiton of the AR(1)
end
```
* Can then construct an instance of that type and access elements through `.`
```julia
ar1 = AR1(0.,0.8,1.) #Note order matters here
println(ar1.ρ)
```

## Adapting function
* We can now adapt our AR(1) function to use that AR1 type
```julia fig_width=10; fig_height=5
"""
   simulateAR1(ar,x0,T)

Simulates an AR(1) ar for T periods with initial value x0
"""
function simulateAR1(ar,x0,T)
    x = zeros(T+1)# initialize
    x[1] = x0
    for t in 1:T
        x[t+1] = (1-ar.ρ)*ar.μ + ar.ρ*x[t] + ar.σ*randn()
    end
    return x[2:end]
end
plot(x=1:100,y=simulateAR1(ar1,0.,100),Geom.line,Guide.XLabel("Time"),Guide.YLabel("AR(1)"))
```

## Unpacking parameters
* OK, that `.` notation is annoying.  Is there an easier way?
    * Of course!  We can use a clever package called `Parameters`

```julia
using Parameters
"""
   simulateAR1(ar,x0,T)

Simulates an AR(1) ar for T periods with initial value x0
"""
function simulateAR1(ar,x0,T)
    @unpack σ,μ,ρ = ar #note order doesn't matter
    x = zeros(T+1)# initialize
    x[1] = x0
    for t in 1:T
        x[t+1] = (1-ρ)*μ + ρ*x[t] + σ*randn()
    end
    return x[2:end]
end
plot(x=1:100,y=simulateAR1(ar1,0.,100),Geom.line,Guide.XLabel("Time"),Guide.YLabel("AR(1)"))
```

## Some Monte Carlo Experiments
* Let's simulate a large sample of AR1 processes
```julia results="hidden"
T = 50
N = 1000
X = zeros(T,N)
for i in 1:N
    X[:,i] .= simulateAR1(ar1,0.,T)
end
```
* Can visualize all the paths
```julia fig_width=10; fig_height=5
plot(X,x=Row.index,y=Col.value,color=Col.index,Geom.line,Guide.XLabel("Time"))
```

## Mean and Standard Deviation
```julia fig_width=10; fig_height=5
#mean(X,dims=2) takes average across rows
hstack(plot(x=1:T,y=mean(X,dims=2),Geom.line,Guide.XLabel("Time"),Guide.YLabel("Mean")),
       plot(x=1:T,y=std(X,dims=2),Geom.line,Guide.XLabel("Time"),Guide.YLabel("Standard Deviation")))
```

# Kalman Filter
## Setup
* The Kalman filter is a famous tool construct an estimate of a hidden state from noisy data

* We assume the following state space system
    * $x_t$ is a $n\times1$ state vector unobservable to the econometrician
    * $y_t$ is a $m\times1$ vector of data then depends on $x_t$ in a noisy manner
$$\begin{aligned}
x_{t+1} =& A x_t + C w_{t+1}\\
y_t =&Gx_t + v_t
\end{aligned}$$
* Where 
    * $w_{t+1}$ is $p\times 1$ an iid sequence of standard normal variables
    * $v_t$ is a iid sequence of normal variables with covaraince $R$

* Initial beliefs
$$
    x_0\sim\mathcal N(\hat x_0,\Sigma_0)
$$
    * We are using $\hat x$ to denote expectation of $x$ given current information
    
## Outline
* We'll approach this recursively:
    * Enter with beliefs $\hat x_0,\Sigma_0$
    * Recieve data: $y_0$
    * Update beliefs: $\hat x_1,\Sigma_1$
    * Repeat.....

* From environment we can back out the anticipated distribution of $y_0$ as
$$
y_0\sim\mathcal N\left(G\hat x_0,G\Sigma_0G' + R\right)
$$ so we know that 
$$
    a_0\equiv y_0 - G\hat x_0 \sim \mathcal N\left(0,G\Sigma_0G' + R\right)
$$

* Question: if we observe $a_0$ how much can we attribute to $x_0$ and how much to $v_0$?

## Some Population Regressions
* Want to construct a relationship
$$
    x_0 - \hat x_0 = L_0a_0 + \eta 
$$ that is unbiased: $\mathbb E\eta = 0$.

* Least squares orthogonality condition implies
$$
\mathbb E(x_0-\hat x_0)a_0' = L_0 \mathbb E a_0a_0'
$$
* Given that $a_0 = G(x_0-\hat x_0) + v_t$ with $v_t$ orthogonal we have
$$
    \Sigma_0G' = L_0\left(G\Sigma_0G' +R\right)
$$or
$$
    L_0 = \Sigma_0G'\left(G\Sigma_0G' +R\right)^{-1}
$$
* This is the first step:
    * Gives: $\mathbb E[x_0|y_0]=\hat x_0 + L_0a_0$
    * Want: $\hat x_1 = \mathbb E[x_1|y_0]$ and $\Sigma_1$

## Updating beliefs
* Our LOM for $x_1$ implies 
$$
    x_1 = A\hat x_0 + A(x_0 -\hat x_0) +C w_1
$$
* Take expectations conditionaly on $y_0$ to get
$$
    \hat x_1 \equiv \mathbb E[x_1|y_0] = A\hat x_0 + AL_0a_0\equiv A \hat x_0 + K_0a_0
$$
* Subtracting these equations gives
$$
    x_1 -\hat x_1 = A(x_0-\hat x_0) +C w_1 - K_0 a_0
$$
* Taking expectations of $(x_1 -\hat x_1)(x_1 -\hat x_1)'$ yields
$$
    \Sigma_1 = (A - K_0G)\Sigma_0(A - K0G)' + CC' + K_0 RK_0' 
$$

## The Kalman Filter
* Combining all of these insights we have the Kalman Filter
$$\begin{align}
    a_t &= y_t - G\hat x_t\\
    K_t &= A\Sigma_t G'(G\Sigma_tG' +R)^{-1}\\
    \hat x_{t+1}&=A\hat x_t + K_t a_t\\
    \Sigma_{t+1} &= CC' + K_t R K_t' + (A - K_tG)\Sigma_t(A-K_tG)'
\end{align}$$

## Implementing The Kalman Filter
```julia results="hidden"
@with_kw mutable struct KalmanFilter #The @with_kw allows us to given default values
    #Parameters
    A::Matrix{Float64} = [0.9][:,:]
    G::Matrix{Float64} = [1.][:,:]
    C::Matrix{Float64} = [1.][:,:]
    R::Matrix{Float64} = [1.][:,:]

    #Initial Beliefs
    x̂0::Vector{Float64} = [0.]
    Σ0::Matrix{Float64} = [1.][:,:]
end
```

## Implementing The Kalman Filter Cont.
```julia results="hidden"
"""
    updateBeliefs(KF::KalmanFilter,y,x̂,Σ)

Uses the Kalman Filter to update beliefs x̂,Σ using data y 
"""
function updateBeliefs(KF::KalmanFilter,y,x̂,Σ)
    @unpack A,G,C,R = KF
    a = y - G*x̂
    K = A*Σ*G'*inv(G*Σ*G'+R)
    x̂′= A*x̂ + K*a
    Σ′ = C*C' + K*R*K' + (A-K*G)*Σ*(A' - G'*K')
    
    return x̂′,Σ′
end
```

## Implementing The Kalman Filter Cont.
```julia results="hidden"
"""
    applyFilter(KF::KalmanFilter,y)

Applies the Kalman Filter on data y. Assume y is mxT where 
T is the number of periods
"""
function applyFilter(KF::KalmanFilter,y)
    @unpack x̂0,Σ0 = KF

    T = size(y,2) #how many rows are y
    x̂ = zeros(length(x̂0),T+1)
    Σ = zeros(length(x̂0),length(x̂0),T+1) #note 3 dimensional array
    x̂[:,1] .= x̂0
    Σ[:,:,1] .= Σ0
    for t in 1:T
        x̂[:,t+1],Σ[:,:,t+1] = updateBeliefs(KF,y[:,t],x̂[:,t],Σ[:,:,t])
    end

    return x̂,Σ
end
```

## Examples
* Let's apply the Kalman filter to the following AR(1)
$$\begin{align}
    x_{t+1} &= 0.95 x_t + w_t\\
    y_t =&= 2x_t + v_t 
\end{align}$$

* Implies $A=[0.95]$, $C=[1.]$, $G=[2.]$, $R=[1.]$

* Set initial beliefs to be ergodic distirbution of $x_t$
    * $\hat x_0 =0$, $\Sigma_0=1/(1-0.95)$

```julia results="hidden"
KF = KalmanFilter(A=[0.95][:,:],C=[1.][:,:], G=[2.][:,:],R=[1.][:,:],x̂0=[0.],Σ0=[1/(1-0.95)][:,:])
```

## Kalman Filter Monte Carlo
```julia fig_width=15; fig_height=5
#First generate data
T = 99
x = simulateAR1(0.0,0.95,1.,1.,T)
y = 2*x .+ randn(T)

#Now perform Kalman filter
x̂,Σ = applyFilter(KF,y') #note we need y to be a row

plot(layer(y=x,Geom.line,color=["True State"]),layer(y=x̂,Geom.line,color=["Filter"]),
        Guide.YLabel("x"),Guide.XLabel("Time"),Guide.colorkey(title=""))
```

## What Happens If We Increase $G$ to 20 from 2?
```julia fig_width=15; fig_height=5
x = simulateAR1(0.0,0.95,1.,1.,T)
y = 20*x .+ randn(T)
KF.G .= 20.
#Now perform Kalman filter
x̂,Σ = applyFilter(KF,y') #note we need y to be a row

plot(layer(y=x,Geom.line,color=["True State"]),layer(y=x̂,Geom.line,color=["Filter"]),
        Guide.YLabel("x"),Guide.XLabel("Time"),Guide.colorkey(title=""))
```


## What Happens If We Decrease $G$ to .2 from 2?
```julia fig_width=15; fig_height=5
x = simulateAR1(0.0,0.95,1.,1.,T)
y = 0.2*x .+ randn(T)
KF.G .= 0.2
#Now perform Kalman filter
x̂,Σ = applyFilter(KF,y') #note we need y to be a row

plot(layer(y=x,Geom.line,color=["True State"]),layer(y=x̂,Geom.line,color=["Filter"]),
        Guide.YLabel("x"),Guide.XLabel("Time"),Guide.colorkey(title=""))
```
# Finite State Processes

## Markov Chains
* Finite State Markov Chains are the building blocks of modern macro

* Idea: random variable $X_t$ takes values in a finite set $X̄$ of length $S$
    * Index values of $\bar X$ by $s=1,2,\ldots,S$
    * Satisfies markov property
    $$
        \text{Pr}\left(X_t|X_{t-1},X_{t-2},\ldots\right) = \text{Pr}\left(X_t|X_{t-1}\right)
    $$

* Therefore $X_t$ is defined entirely by
    * $\bar X$: A vector of values for each states 
    * $\pi_0$: An initial probability distribution for $X_0$
    * $P$: A transition matrix that records
    $$
    P_{ij} = \text{Pr}\left(X_t=\bar X[j]|X_t = \bar X[i]\right)
    $$

## Properties of Markov Chains
* Conditional expectations are matrix multiplacation
$$
    \mathbb E\left[X_{t+1}|X_t=\bar X[i]\right] = (P\bar X)_i
$$and
$$
    \mathbb E\left[X_{t+k}|X_t=\bar X[i]\right] = (P^k\bar X)_i
$$ 
* Conditional probabilities are easy to compute
$$
    \text{Pr}\left[X_{t+k}=\bar X[j]|X_t=\bar X[i]\right] = (P^k)_{ij}
$$ 
* Computing unconditional distirbution is matrix multiplication
$$
    \pi_t' = \pi_0'P^t 
$$
* Stationary distributions $\pi^*$ are left-eigenvectors
$$
    (\pi^*)'=(\pi^*)'P
$$

## Simulating Markov Chain

* We could spend time creating our own function to simulate markov Chains

* Or we could use something prepackaged (don't reinvent the wheel)

* QuantEcon has a lot of helpful libraries
```julia results = "hidden"
using QuantEcon
```

* In this case we can use `simulate`
```julia
P = [0.6 0.4;
     0.4 0.6]
s = simulate(MarkovChain(P),100,init=1)
println(s)
```

## Rouwenhorst 

* QuantEcon also has a pre-packaged Rouwenhorst method
    * Approximates an AR(1) (see homework)
```julia results = "hidden"
mc_ar1 = rouwenhorst(51,0.9,0.014)
```
* Let's try some monte carlo 
```julia
X = zeros(15,1000)
for i in 1:1000
    X[:,i] = simulate(mc_ar1,15,init=1)
end
println(mean(X[15,:]))
```

* Compare to formula
```julia
P,X̄ = mc_ar1.p,mc_ar1.state_values
println((P^14*X̄)[1])
```

## Long Run Stationary Distribution
* We can compute the stationary distribution of this Chain
* Two ways:
```julia
using LinearAlgebra
D,V = eigen(P')  #should be left unit eigenvector
πstar = V[:,isapprox.(D,1)][:]
πstar ./= sum(πstar)#Need to normalize for probability

#or 

πstar2 = (P^200)[15,:] #probability distribution 1000 periods in the future
println(norm(πstar -πstar2))
```
* Surprisingly the second can often be faster (need to check how many periods)
```julia
@time D,V = eigen(P');
@time πstar2 = (P^200)[15,:];
```

## Compare to Monte Carlo
```julia fig_width=10; fig_height=5
s_end = zeros(Int,10000)
for i in 1:10000
    s_end[i] = simulate_indices(mc_ar1,200,init=1)[end]
end

plot(layer(x=s_end,Geom.histogram(density=true,bincount=51)),
     layer(x=1:51,y=πstar2,color=[colorant"red"],Geom.point,order=1)
    ,Guide.XLabel("State"),Guide.YLabel("Probability"))
```
